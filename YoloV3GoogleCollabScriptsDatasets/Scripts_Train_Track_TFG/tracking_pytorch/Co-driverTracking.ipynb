{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Co-driverTracking.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"IiGSK7j78uAT","colab_type":"code","outputId":"c1fdd380-fa81-427d-92cd-930e07715ea9","executionInfo":{"status":"ok","timestamp":1582803059274,"user_tz":-60,"elapsed":26457,"user":{"displayName":"Urtzi Sotes","photoUrl":"","userId":"05447564087545297430"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["# This cell imports the drive library and mounts your Google Drive as a VM local drive. You can access to your Drive files \n","# using this path \"/content/gdrive/My Drive/\"\n","# Hay que ejecutarlo cada vez que quiera probar esto\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F6hSVZKkBOMw","colab_type":"code","outputId":"e87e5898-7eea-477d-dcff-3a2a3247814f","executionInfo":{"status":"ok","timestamp":1582803073256,"user_tz":-60,"elapsed":11459,"user":{"displayName":"Urtzi Sotes","photoUrl":"","userId":"05447564087545297430"}},"colab":{"base_uri":"https://localhost:8080/","height":479}},"source":["# Clonamos de mi repositorio de git los scripts necesarios para ejecutar esto\n","!git clone https://github.com/urtzi98/pytorch_objectDetectTrack.git\n","!pip install filterpy"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'pytorch_objectDetectTrack'...\n","remote: Enumerating objects: 27, done.\u001b[K\n","remote: Counting objects:   3% (1/27)\u001b[K\rremote: Counting objects:   7% (2/27)\u001b[K\rremote: Counting objects:  11% (3/27)\u001b[K\rremote: Counting objects:  14% (4/27)\u001b[K\rremote: Counting objects:  18% (5/27)\u001b[K\rremote: Counting objects:  22% (6/27)\u001b[K\rremote: Counting objects:  25% (7/27)\u001b[K\rremote: Counting objects:  29% (8/27)\u001b[K\rremote: Counting objects:  33% (9/27)\u001b[K\rremote: Counting objects:  37% (10/27)\u001b[K\rremote: Counting objects:  40% (11/27)\u001b[K\rremote: Counting objects:  44% (12/27)\u001b[K\rremote: Counting objects:  48% (13/27)\u001b[K\rremote: Counting objects:  51% (14/27)\u001b[K\rremote: Counting objects:  55% (15/27)\u001b[K\rremote: Counting objects:  59% (16/27)\u001b[K\rremote: Counting objects:  62% (17/27)\u001b[K\rremote: Counting objects:  66% (18/27)\u001b[K\rremote: Counting objects:  70% (19/27)\u001b[K\rremote: Counting objects:  74% (20/27)\u001b[K\rremote: Counting objects:  77% (21/27)\u001b[K\rremote: Counting objects:  81% (22/27)\u001b[K\rremote: Counting objects:  85% (23/27)\u001b[K\rremote: Counting objects:  88% (24/27)\u001b[K\rremote: Counting objects:  92% (25/27)\u001b[K\rremote: Counting objects:  96% (26/27)\u001b[K\rremote: Counting objects: 100% (27/27)\u001b[K\rremote: Counting objects: 100% (27/27), done.\u001b[K\n","remote: Compressing objects: 100% (27/27), done.\u001b[K\n","remote: Total 27 (delta 0), reused 27 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (27/27), done.\n","Collecting filterpy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/ac8914360460fafa1990890259b7fa5ef7ba4cd59014e782e4ab3ab144d8/filterpy-1.4.5.zip (177kB)\n","\u001b[K     |████████████████████████████████| 184kB 5.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from filterpy) (1.17.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from filterpy) (1.4.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from filterpy) (3.1.3)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->filterpy) (2.6.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->filterpy) (2.4.6)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->filterpy) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->filterpy) (1.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->filterpy) (1.12.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->filterpy) (45.1.0)\n","Building wheels for collected packages: filterpy\n","  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for filterpy: filename=filterpy-1.4.5-cp36-none-any.whl size=110451 sha256=8c5ec8cf616f68d5a7811f3cdf53988086bc97f67cf8a7c6b861920f091b7aa3\n","  Stored in directory: /root/.cache/pip/wheels/c3/0c/dd/e92392c3f38a41371602d99fc77d6c1d42aadbf0c6afccdd02\n","Successfully built filterpy\n","Installing collected packages: filterpy\n","Successfully installed filterpy-1.4.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GC6gA_oGOI_r","colab_type":"code","outputId":"5a416a2c-9e10-4382-9e2c-c0d1668a4bea","executionInfo":{"status":"ok","timestamp":1582803097781,"user_tz":-60,"elapsed":17120,"user":{"displayName":"Urtzi Sotes","photoUrl":"","userId":"05447564087545297430"}},"colab":{"base_uri":"https://localhost:8080/","height":276}},"source":["# We're unzipping the cuDNN files from your Drive folder directly to the VM CUDA folders\n","!tar -xzvf gdrive/My\\ Drive/darknet/cuDNN/cudnn-10.0-linux-x64-v7.5.0.56.tgz -C /usr/local/\n","!chmod a+r /usr/local/cuda/include/cudnn.h\n","!cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2\n","%cd pytorch_objectDetectTrack/"],"execution_count":3,"outputs":[{"output_type":"stream","text":["cuda/include/cudnn.h\n","cuda/NVIDIA_SLA_cuDNN_Support.txt\n","cuda/lib64/libcudnn.so\n","cuda/lib64/libcudnn.so.7\n","cuda/lib64/libcudnn.so.7.5.0\n","cuda/lib64/libcudnn_static.a\n","#define CUDNN_MAJOR 7\n","#define CUDNN_MINOR 5\n","#define CUDNN_PATCHLEVEL 0\n","--\n","#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n","\n","#include \"driver_types.h\"\n","/content/pytorch_objectDetectTrack\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3cqcD31itQPP","colab_type":"code","outputId":"9cc82890-1eae-479c-926a-00e5ab40c6f8","executionInfo":{"status":"ok","timestamp":1582559513438,"user_tz":-60,"elapsed":2641,"user":{"displayName":"Urtzi Sotes","photoUrl":"","userId":"05447564087545297430"}},"colab":{"base_uri":"https://localhost:8080/","height":721}},"source":["#principal program\n","from models import *\n","from utils import *\n","\n","import os, sys, time, datetime, random\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","\n","from PIL import Image\n","\n","# load weights and set defaults\n","config_path='/content/gdrive/My Drive/darknet/cfg/yolov3-bdd100k.cfg'\n","weights_path='/content/gdrive/My Drive/darknet/backup/3clasesAnchorsSubd16/yolov3-bdd100k_6000.weights'\n","class_path='/content/gdrive/My Drive/darknet/cfg/bdd100k-3clases.names'\n","img_size=416\n","conf_thres=0.7\n","nms_thres=0.3\n","\n","# load model and put into eval mode\n","model = Darknet(config_path, img_size=img_size)\n","model.load_weights(weights_path)\n","model.cuda()\n","model.eval()\n","\n","classes = utils.load_classes(class_path)\n","Tensor = torch.cuda.FloatTensor\n","\n","def detect_image(img):\n","    # scale and pad image\n","    ratio = min(img_size/img.size[0], img_size/img.size[1])\n","    imw = round(img.size[0] * ratio)\n","    imh = round(img.size[1] * ratio)\n","    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n","         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n","                        (128,128,128)),\n","         transforms.ToTensor(),\n","         ])\n","    # convert image to Tensor\n","    image_tensor = img_transforms(img).float()\n","    image_tensor = image_tensor.unsqueeze_(0)\n","    input_img = Variable(image_tensor.type(Tensor))\n","    # run inference on the model and get detections\n","    with torch.no_grad():\n","        detections = model(input_img)\n","        detections = utils.non_max_suppression(detections, 80, conf_thres, nms_thres)\n","    return detections[0]\n","\n","videopath = '/content/gdrive/My Drive/darknet/data/calleVehiculos_1.mp4'\n","\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","import cv2\n","from sort import *\n","colors=[(255,0,0),(0,255,0),(0,0,255),(255,0,255),(128,0,0),(0,128,0),(0,0,128),(128,0,128),(128,128,0),(0,128,128)]\n","\n","vid = cv2.VideoCapture(videopath)\n","mot_tracker = Sort() \n","\n","#cv2.namedWindow('Stream',cv2.WINDOW_NORMAL)\n","#cv2.resizeWindow('Stream', (800,600))\n","\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","ret,frame=vid.read()\n","vw = frame.shape[1]\n","vh = frame.shape[0]\n","print (\"Video size\", vw,vh)\n","dim = (800,600)\n","outvideo = cv2.VideoWriter(videopath.replace(\".mp4\", \"-result.mp4\"),fourcc,20.0,(400,350))\n","\n","frames = 0\n","starttime = time.time()\n","#obj_id=0\n","frame_threshold = 4\n","while(True):\n","    ret, frame = vid.read()\n","    if not ret:\n","        break\n","    if (np.mod(frames,frame_threshold) == 0 and frames != 8 and frames !=12):\n","        frame = cv2.resize(frame,dim,interpolation = cv2.INTER_AREA) ##hacemos un resize para quitar resolucion\n","        frame = frame[250:601,25:421] #con esto recortamos la imagen\n","        #frames += 1\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        pilimg = Image.fromarray(frame)\n","        detections = detect_image(pilimg)\n","\n","        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n","        img = np.array(pilimg)\n","        pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n","        pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n","        unpad_h = img_size - pad_y\n","        unpad_w = img_size - pad_x\n","        if detections is not None:\n","            tracked_objects = mot_tracker.update(detections.cpu())\n","\n","            unique_labels = detections[:, -1].cpu().unique()\n","            n_cls_preds = len(unique_labels)\n","            print(\"En el frame \",frames,\"Encontramos los siguientes objectsID\")\n","            #print(unique_labels)\n","            for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n","                box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n","                box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n","                y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n","                x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n","                color = colors[int(obj_id) % len(colors)]\n","                cls = classes[int(cls_pred)]\n","                cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 4)\n","                cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+80, y1), color, -1)\n","                cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n","                print(obj_id)\n","                #cv2.imshow('Stream', frame)\n","                #plt.imshow(frame)\n","                #plt.show()\n","        outvideo.write(frame)\n","        ch = 0xFF & cv2.waitKey(1)\n","        if ch == 27:\n","            break\n","    frames += 1\n","totaltime = time.time()-starttime\n","print(frames, \"frames\", totaltime/frames, \"s/frame\")\n","print(\"Total time to process de vide is \",totaltime)\n","cv2.destroyAllWindows()\n","outvideo.release()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Video size 1920 1080\n","En el frame  0 Encontramos los siguientes objectsID\n","22.0\n","21.0\n","En el frame  4 Encontramos los siguientes objectsID\n","22.0\n","21.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["En el frame  16 Encontramos los siguientes objectsID\n","24.0\n","23.0\n","22.0\n","21.0\n","En el frame  20 Encontramos los siguientes objectsID\n","22.0\n","21.0\n","En el frame  24 Encontramos los siguientes objectsID\n","22.0\n","21.0\n","En el frame  28 Encontramos los siguientes objectsID\n","23.0\n","22.0\n","21.0\n","En el frame  32 Encontramos los siguientes objectsID\n","23.0\n","22.0\n","En el frame  36 Encontramos los siguientes objectsID\n","23.0\n","22.0\n","En el frame  40 Encontramos los siguientes objectsID\n","23.0\n","22.0\n","43 frames 0.025440598643103312 s/frame\n","Total time to process de vide is  1.0939457416534424\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AoTzmpa8fmoT","colab_type":"code","outputId":"246729aa-7f9d-465d-91f4-2f45ed2e51e5","executionInfo":{"status":"ok","timestamp":1582536138714,"user_tz":-60,"elapsed":2626,"user":{"displayName":"Urtzi Sotes","photoUrl":"","userId":"05447564087545297430"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["config\tmodels.py    PyTorch_Object_Detection.ipynb  README.md\tutils\n","images\t__pycache__  PyTorch_Object_Tracking.ipynb   sort.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jincWiGDflqa","colab_type":"code","outputId":"a4ce4f34-fbe7-4aef-b1c4-2c0d6211a798","executionInfo":{"status":"ok","timestamp":1582555487765,"user_tz":-60,"elapsed":675,"user":{"displayName":"Urtzi Sotes","photoUrl":"","userId":"05447564087545297430"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["vid = cv2.VideoCapture(videopath)\n","frame_count = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n","fps = vid.get(cv2.CAP_PROP_FPS) #frames per second\n","duration = frame_count/fps\n","print(str(fps))\n","print(duration)\n","vid.release()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["30.0\n","1.4666666666666666\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FxEn-HssiPu2","colab_type":"code","colab":{}},"source":["from google.colab import files\n","files.download('road-res.mp4')"],"execution_count":0,"outputs":[]}]}